{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bccb32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "91\n",
      "158\n",
      "193\n",
      "192\n",
      "['Jacques Médecin', 'Saracens', 'Antisemitism', 'Mediterranean Sea', 'Promenade des Anglais']\n",
      "\n",
      "\n",
      "Nice (/niːs/ NEESS, French pronunciation: [nis] (listen); Niçard: Niça, classical norm, or Nissa, nonstandard, pronounced [ˈnisa]; Italian: Nizza [ˈnittsa]; Ligurian: Nissa; Ancient Greek: Νίκαια; Latin: Nicaea) is the prefecture of the Alpes-Maritimes department in France. The Nice agglomeration extends far beyond the administrative city limits, with a population of nearly 1 million[4][3] on an area of 744 km2 (287 sq mi).[3] Located on the French Riviera, the southeastern coast of France on the Mediterranean Sea, at the foot of the French Alps, Nice is the second-largest French city on the Mediterranean coast and second-largest city in the Provence-Alpes-Côte d'Azur region after Marseille. Nice is approximately 13 kilometres (8 mi) from the principality of Monaco and 30 kilometres (19 mi) from the French–Italian border. Nice's airport serves as a gateway to the region.\n",
      "\n",
      "The city is nicknamed Nice la Belle (Nissa La Bella in Niçard), meaning 'Nice the Beautiful', which is also the title of the unofficial anthem of Nice, written by Menica Rondelly in 1912. The area of today's Nice contains Terra Amata, an archaeological site which displays evidence of a very early use of fire 380,000 years ago. Around 350 BC, Greeks of Marseille founded a permanent settlement and called it Νίκαια, Nikaia, after Nike, the goddess of victory.[5] Through the ages, the town has changed hands many times. Its strategic location and port significantly contributed to its maritime strength. From 1388 it was a dominion of Savoy, then became part of the French First Republic between 1792 and 1815, when it was returned to the Kingdom of Piedmont-Sardinia, the legal predecessor of the Kingdom of Italy, until its re-annexation by France in 1860.\n",
      "\n",
      "The natural environment of the Nice area and its mild Mediterranean climate came to the attention of the English upper classes in the second half of the 18th century, when an increasing number of aristocratic families took to spending their winters there. In 1931, following its refurbishment the city's main seaside promenade, the Promenade des Anglais (\"Walkway of the English\"), was inaugurated by Prince Arthur, Duke of Connaught; it owes its name to visitors to the resort.[6] These included Queen Victoria along with her son Edward VII who spent winters there, as well as Henry Cavendish, born in Nice, who discovered hydrogen.\n",
      "\n",
      "The clear air and soft light have particularly appealed to notable painters, such as Marc Chagall, Henri Matisse, Niki de Saint Phalle and Arman. Their work is commemorated in many of the city's museums, including Musée Marc Chagall, Musée Matisse and Musée des Beaux-Arts.[7] International writers have also been attracted and inspired by the city. Frank Harris wrote several books including his autobiography My Life and Loves in Nice. Friedrich Nietzsche spent six consecutive winters in Nice, and wrote Thus Spoke Zarathustra here. Additionally, Russian writer Anton Chekhov completed his play Three Sisters while living in Nice.\n",
      "\n",
      "Nice's appeal extended to the Russian upper classes. Prince Nicholas Alexandrovich, heir apparent to Imperial Russia, died in Nice and was a patron of the Russian Orthodox Cemetery, Nice where Princess Catherine Dolgorukova, morganatic wife of the Tsar Alexander II of Russia, is buried. Also buried there are General Dmitry Shcherbachev and General Nikolai Yudenich, leaders of the anti-Communist White Movement.\n",
      "\n",
      "Those interred at the Cimetière du Château include celebrated jeweler Alfred Van Cleef, Emil Jellinek-Mercedes, founder of the Mercedes car company, film director Louis Feuillade, poet Agathe-Sophie Sasserno, dancer Carolina Otero, Asterix comics creator René Goscinny, The Phantom of the Opera author Gaston Leroux, French prime minister Léon Gambetta, and the first president of the International Court of Justice José Gustavo Guerrero.\n",
      "\n",
      "Because of its historical importance as a winter resort town for the European aristocracy and the resulting mix of cultures found in the city, UNESCO proclaimed Nice a World Heritage Site in 2021.[8] The city has the second largest hotel capacity in the country,[9] and it is one of its most visited cities, receiving 4 million tourists every year.[10] It also has the third busiest airport in France, after the two main Parisian ones.[11] It is the historical capital city of the County of Nice (French: Comté de Nice, Niçard: Countèa de Nissa).[12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "## Using BeautifulSoup and requests modules.\n",
    "\n",
    "test1 = requests.get(\"https://en.wikipedia.org/wiki/Nice\").text\n",
    "souptest = BeautifulSoup(test1, \"lxml\")\n",
    "## I'm grabbing a page off Wikipedia directly.\n",
    "\n",
    "from random import *\n",
    "## I need an arbitrary function from random, that being randint()\n",
    "\n",
    "sectiontest = souptest.find_all(\"p\")\n",
    "## Find every paragraph in the HTML of souptest.\n",
    "\n",
    "a_title = []\n",
    "## a_title is a list of every <a> tag in every paragraph of the page, HTML format.\n",
    "for i in sectiontest:\n",
    "    try:\n",
    "        a_title.append(i.find_all(\"a\"))\n",
    "        ## Appending every <a> tag into the list. These are written in HTML format including both href and title.\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    a_links = []\n",
    "    ## a_links is a list of every link pulled from the <a> strings in a_title.\n",
    "    for i in a_title:\n",
    "        for j in i:\n",
    "            if j.get(\"title\") == None:\n",
    "                pass\n",
    "            else:\n",
    "                a_links.append(j.get(\"title\"))\n",
    "                ## While I'm pulling href here I can also pull titles straight out.          \n",
    "\n",
    "randomlinks = []\n",
    "## This is a list of (currently) five random links from a_links\n",
    "x = 0\n",
    "while x != 5:\n",
    "    random = randint(0,len(a_links)-1)\n",
    "    print(random)\n",
    "    ## This is to know which index I'm pulling from.\n",
    "    randomlinks.append(a_links[random])\n",
    "    ## Appending the corresponding index into the list.\n",
    "    x+= 1\n",
    "    \n",
    "print(randomlinks)\n",
    "    \n",
    "div = souptest.find(\"div\", class_=\"toc\")\n",
    "## This universally finds the table of contents from a Wikipedia page, as all table of contexts have the class \"toc\"\n",
    "summary = div.find_previous_siblings(\"p\")\n",
    "## This command will find all previous \"siblings\" or tags within the same div, in this case <p> tags are being pulled.\n",
    "for i in list(reversed(summary)):\n",
    "    ## .find_previous_siblings() always searches from the bottom up, thus printing in reverse.\n",
    "    print(i.text)\n",
    "    ## I'm using the .text method to take each <p> tag out of HTML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe1519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math\n",
      "269\n",
      "109\n",
      "365\n",
      "['/wiki/Risk', '/wiki/Algebraic_geometry', '/wiki/Bulletin_of_the_American_Mathematical_Society']\n",
      "/wiki/Risk\n",
      "116\n",
      "21\n",
      "3\n",
      "['/wiki/Human_factors', '/wiki/Environmental_hazards', '/wiki/Environmental_science']\n",
      "/wiki/Algebraic_geometry\n",
      "77\n",
      "240\n",
      "37\n",
      "['/wiki/Dual_(category_theory)', '/wiki/Algebraic_variety', '/wiki/Hilbert%27s_Nullstellensatz']\n",
      "/wiki/Bulletin_of_the_American_Mathematical_Society\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "['/wiki/Institute_for_Scientific_Information', '/wiki/Science_Citation_Index', '/wiki/Institute_for_Scientific_Information', '/wiki/Science_Citation_Index']\n",
      "[['/wiki/Human_factors', '/wiki/Environmental_hazards', '/wiki/Environmental_science'], ['/wiki/Dual_(category_theory)', '/wiki/Algebraic_variety', '/wiki/Hilbert%27s_Nullstellensatz'], ['/wiki/Institute_for_Scientific_Information', '/wiki/Science_Citation_Index', '/wiki/Institute_for_Scientific_Information', '/wiki/Science_Citation_Index']]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from random import randint\n",
    "\n",
    "undesirable = [\n",
    "    '/wiki/Wikipedia:Please_clarify',\n",
    "    '/wiki/Wikipedia:Citation_needed',\n",
    "    '/w/',\n",
    "    'disambiguation',\n",
    "    'wiktionary',\n",
    "    '/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items',\n",
    "    '/wiki/Help:IPA/English'\n",
    "]\n",
    "\n",
    "class Wikipedia:\n",
    "    def __init__(self, search_term):\n",
    "        self.search_term = search_term\n",
    "        \n",
    "    def test(self):\n",
    "        print(self.search_term)\n",
    "        address = f\"/wiki/{self.search_term}\"\n",
    "        print(address)\n",
    "        \n",
    "    def titles(self):\n",
    "        address = f\"/wiki/{self.search_term}\"\n",
    "        ## This converts the search term to a format Wikipedia will recognise.\n",
    "        site = requests.get(f\"https://en.wikipedia.org/{address}\").text\n",
    "        soupsite = BeautifulSoup(site, \"lxml\")\n",
    "        ## Requesting website.\n",
    "        \n",
    "        paragraphs = soupsite.find_all(\"p\")\n",
    "        ## Generating a list of every paragraph.\n",
    "\n",
    "        a_tags = []\n",
    "        for i in paragraphs:\n",
    "            try:\n",
    "                a_tags.append(i.find_all(\"a\"))\n",
    "                ## Generating a list of every <a> tag.\n",
    "            except:\n",
    "                ## The try-except condition is a failsafe for any paragraph not containing any <a> tags.\n",
    "                ## To be honest I forgot what this did, but it works. \n",
    "                pass\n",
    "            \n",
    "            a_titles = []\n",
    "            for i in a_tags:\n",
    "                for j in i:\n",
    "                    if j.get(\"title\") == None:\n",
    "                        ## Some <a> tags contain no titles whatsoever, so I'm filtering them out to prevent issues later on.\n",
    "                        pass\n",
    "                    else:\n",
    "                        a_titles.append(j.get(\"title\"))\n",
    "                        ## Grabbing the titles from every <a> tag.\n",
    "                                \n",
    "        print(a_titles)\n",
    "        \n",
    "    def links(self):\n",
    "        ## Same as titles(self)\n",
    "        address = f\"/wiki/{self.search_term}\"\n",
    "        site = requests.get(f\"https://en.wikipedia.org{address}\").text\n",
    "        soupsite = BeautifulSoup(site, \"lxml\")\n",
    "\n",
    "        paragraphs = soupsite.find_all(\"p\")\n",
    "        \n",
    "        a_tags = []\n",
    "        for i in paragraphs:\n",
    "            try:\n",
    "                a_tags.append(i.find_all(\"a\"))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            a_links = []\n",
    "            ## Different variable name in case I want to make both above's and this one's global.\n",
    "            for i in a_tags:\n",
    "                for j in i:\n",
    "                    if j.get(\"title\") == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        a_links.append(j.get(\"href\"))\n",
    "                        ## Grabbing the links from every <a> tag.\n",
    "        \n",
    "        print(a_links)\n",
    "        print(len(a_links))\n",
    "        \n",
    "    def summary(self):\n",
    "        address = f\"/wiki/{self.search_term}\"\n",
    "        site = requests.get(f\"https://en.wikipedia.org{address}\").text\n",
    "        soupsite = BeautifulSoup(site, \"lxml\")\n",
    "        \n",
    "        div = soupsite.find(\"div\", class_=\"toc\")\n",
    "        ## Wikipedia universally structures its webpages so that a \"div\" or section with the class \"toc\" represents the Table of Contents.\n",
    "        ## Naturally, everything paragraph above the Table of Contents should be the summary.\n",
    "        summary = div.find_previous_siblings(\"p\")\n",
    "        for i in list(reversed(summary)):\n",
    "            ## I use \"reversed()\" because the \".find_previous_siblings()\" method reads bottom to top,\n",
    "            ## which returns the summary backwards.\n",
    "            print(i.text.strip())\n",
    "        \n",
    "    def randomlinks(self, iterations):\n",
    "        address = f\"/wiki/{self.search_term}\"\n",
    "        site = requests.get(f\"https://en.wikipedia.org{address}\").text\n",
    "        soupsite = BeautifulSoup(site, \"lxml\")\n",
    "\n",
    "        paragraphs = soupsite.find_all(\"p\")\n",
    "        \n",
    "        a_tags = []\n",
    "        for i in paragraphs:\n",
    "            try:\n",
    "                a_tags.append(i.find_all(\"a\"))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            a_links = []\n",
    "            for i in a_tags:\n",
    "                for j in i:\n",
    "                    if j.get(\"title\") == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        a_links.append(j.get(\"href\"))\n",
    "        \n",
    "        ## This plays out the same way as all previous executions.\n",
    "                        \n",
    "        x = 0\n",
    "        global random_links\n",
    "        ## Setting this list as a global variable allows me access to it any time.\n",
    "        random_links = []\n",
    "        while x != iterations:\n",
    "            random_place = randint(1,(len(a_links)-1))\n",
    "            print(random_place)\n",
    "            if any(i in a_links[random_place] for i in undesirable):\n",
    "                random_links.append(a_links[random_place+1])\n",
    "            else:\n",
    "                random_links.append(a_links[random_place])\n",
    "            x += 1\n",
    "        print(random_links)\n",
    "        ## I just need a readout of some number of randomly generated links to create the web. \n",
    "        \n",
    "    def dungeonlinks(self, iterations):\n",
    "        site = requests.get(f\"https://en.wikipedia.org{self.search_term}\").text\n",
    "        soupsite = BeautifulSoup(site, \"lxml\")\n",
    "\n",
    "        paragraphs = soupsite.find_all(\"p\")\n",
    "        \n",
    "        a_tags = []\n",
    "        for i in paragraphs:\n",
    "            try:\n",
    "                a_tags.append(i.find_all(\"a\"))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            a_links = []\n",
    "            for i in a_tags:\n",
    "                for j in i:\n",
    "                    if j.get(\"title\") == None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        a_links.append(j.get(\"href\"))\n",
    "        \n",
    "        ## This plays out the same way as all previous executions.\n",
    "                        \n",
    "        x = 0\n",
    "        global random_links\n",
    "        ## Setting this list as a global variable allows me access to it any time.\n",
    "        random_links = []\n",
    "        while x != iterations:\n",
    "            random_place = randint(1,(len(a_links)-1))\n",
    "            print(random_place)\n",
    "            if any(i in a_links[random_place] for i in undesirable):\n",
    "                ## The any() function will actually search strings iteratively for strings in a list.\n",
    "                random_links.append(a_links[random_place+1])\n",
    "            else:\n",
    "                random_links.append(a_links[random_place])\n",
    "            x += 1\n",
    "        print(random_links)\n",
    "        ## I just need a readout of some number of randomly generated links to create the web. \n",
    "                \n",
    "    def rand_summary(self):\n",
    "        for i in random_links:\n",
    "            site = requests.get(f\"https://en.wikipedia.org{i}\").text\n",
    "            soupsite = BeautifulSoup(site, \"lxml\")\n",
    "            \n",
    "            div = soupsite.find(\"div\", class_=\"toc\")\n",
    "            summary = div.find_previous_siblings(\"p\")\n",
    "            for j in list(reversed(summary)):\n",
    "                print(j.text)\n",
    "        \n",
    "def dungeon_crawler(search_term, seed):\n",
    "    print(search_term)\n",
    "    room_one = Wikipedia(search_term)\n",
    "    try:\n",
    "        room_one.randomlinks(seed)\n",
    "        ## For first run\n",
    "    except:\n",
    "        room_one.dungeonlinks(seed)\n",
    "        ## For iteration\n",
    "    global new_rooms\n",
    "    new_rooms = []\n",
    "    for i in random_links:\n",
    "        print(i)\n",
    "        next_room = Wikipedia(i)\n",
    "        number = randint(1, seed+1)\n",
    "        ## Variables for the following methods.\n",
    "        next_room.dungeonlinks(number)\n",
    "        new_rooms.append(random_links)\n",
    "        ## random_links gets reassigned each loop and folded into new_rooms\n",
    "    print(new_rooms)\n",
    "\n",
    "dungeon_crawler(\"Math\",3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ffae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Human_factors\n",
      "108\n",
      "96\n",
      "27\n",
      "['/wiki/Usability', '/wiki/Personal_protective_equipment', '/wiki/Ancient_Greek']\n",
      "/wiki/Usability\n",
      "82\n",
      "47\n",
      "['/wiki/Alan_Cooper', '/wiki/Intuition_(knowledge)']\n",
      "/wiki/Personal_protective_equipment\n",
      "35\n",
      "49\n",
      "60\n",
      "['/wiki/Health_and_Safety_Executive', '/wiki/Face_shield', '/wiki/Hearing_protection_fit-testing']\n",
      "/wiki/Ancient_Greek\n",
      "114\n",
      "72\n",
      "112\n",
      "146\n",
      "['/wiki/Homer', '/wiki/Fricative_consonant', '/wiki/Augment_(Indo-European)', '/wiki/Federal_Statistical_Office_of_Germany']\n",
      "[['/wiki/Alan_Cooper', '/wiki/Intuition_(knowledge)'], ['/wiki/Health_and_Safety_Executive', '/wiki/Face_shield', '/wiki/Hearing_protection_fit-testing'], ['/wiki/Homer', '/wiki/Fricative_consonant', '/wiki/Augment_(Indo-European)', '/wiki/Federal_Statistical_Office_of_Germany']]\n",
      "/wiki/Environmental_hazards\n",
      "12\n",
      "7\n",
      "30\n",
      "['/wiki/Environmental_impact_assessment', '/wiki/Water_pollution', '/wiki/Human_impact_on_the_environment']\n",
      "/wiki/Environmental_impact_assessment\n",
      "19\n",
      "119\n",
      "78\n",
      "['/wiki/National_Environmental_Policy_Act', '/wiki/Space_debris', '/wiki/Resource_Management_Act_1991']\n",
      "/wiki/Water_pollution\n",
      "135\n",
      "202\n",
      "['/wiki/Electrical_conductance', '/wiki/Silt_fence']\n",
      "/wiki/Human_impact_on_the_environment\n",
      "164\n",
      "52\n",
      "58\n",
      "['/wiki/Coral_bleaching', '/wiki/Rodolfo_Dirzo', '/wiki/Soil_degradation']\n",
      "[['/wiki/National_Environmental_Policy_Act', '/wiki/Space_debris', '/wiki/Resource_Management_Act_1991'], ['/wiki/Electrical_conductance', '/wiki/Silt_fence'], ['/wiki/Coral_bleaching', '/wiki/Rodolfo_Dirzo', '/wiki/Soil_degradation']]\n",
      "/wiki/Environmental_science\n",
      "104\n",
      "12\n",
      "63\n",
      "['/wiki/Wildlife_management', '/wiki/Soil_science', '/wiki/United_Nations']\n",
      "/wiki/Wildlife_management\n",
      "72\n",
      "48\n",
      "2\n",
      "['/wiki/University_of_Wisconsin,_Madison', '/wiki/Manchester', '/wiki/Human']\n",
      "/wiki/Soil_science\n",
      "50\n",
      "49\n",
      "['/wiki/Rugged_computer', '/wiki/LiDAR']\n",
      "/wiki/United_Nations\n",
      "393\n",
      "231\n",
      "250\n",
      "['/wiki/COVID-19_pandemic_in_Taiwan', '/wiki/The_Hague', '/wiki/United_Nations_General_Assembly_observers']\n",
      "[['/wiki/University_of_Wisconsin,_Madison', '/wiki/Manchester', '/wiki/Human'], ['/wiki/Rugged_computer', '/wiki/LiDAR'], ['/wiki/COVID-19_pandemic_in_Taiwan', '/wiki/The_Hague', '/wiki/United_Nations_General_Assembly_observers']]\n",
      "/wiki/Dual_(category_theory)\n",
      "13\n",
      "10\n",
      "1\n",
      "['/wiki/De_Morgan%27s_laws', '/wiki/Morphism', '/wiki/Mathematics']\n",
      "/wiki/De_Morgan%27s_laws\n",
      "28\n",
      "['/wiki/William_of_Ockham']\n",
      "/wiki/Morphism\n",
      "20\n",
      "39\n",
      "9\n",
      "2\n",
      "['/wiki/Function_(mathematics)', '/wiki/Category_of_sets', '/wiki/Group_homomorphism', '/wiki/Map_(mathematics)']\n",
      "/wiki/Mathematics\n",
      "512\n",
      "44\n",
      "['/wiki/Apery%27s_theorem', '/wiki/Infinitesimal_calculus']\n",
      "[['/wiki/William_of_Ockham'], ['/wiki/Function_(mathematics)', '/wiki/Category_of_sets', '/wiki/Group_homomorphism', '/wiki/Map_(mathematics)'], ['/wiki/Apery%27s_theorem', '/wiki/Infinitesimal_calculus']]\n",
      "/wiki/Algebraic_variety\n",
      "47\n",
      "79\n",
      "102\n",
      "['/wiki/Regular_function', '/wiki/General_linear_group', '/wiki/Abelian_variety']\n",
      "/wiki/Regular_function\n",
      "46\n",
      "41\n",
      "17\n",
      "['/wiki/Projective_variety', '/wiki/Zariski%27s_main_theorem', '/wiki/Coordinate_ring']\n",
      "/wiki/General_linear_group\n",
      "115\n",
      "['/wiki/Affine_group']\n",
      "/wiki/Abelian_variety\n",
      "100\n",
      "63\n",
      "41\n",
      "36\n",
      "['/wiki/Weil_pairing', '/wiki/Genus_(mathematics)', '/wiki/Andr%C3%A9_Weil', '/wiki/Karl_Weierstrass']\n",
      "[['/wiki/Projective_variety', '/wiki/Zariski%27s_main_theorem', '/wiki/Coordinate_ring'], ['/wiki/Affine_group'], ['/wiki/Weil_pairing', '/wiki/Genus_(mathematics)', '/wiki/Andr%C3%A9_Weil', '/wiki/Karl_Weierstrass']]\n",
      "/wiki/Hilbert%27s_Nullstellensatz\n",
      "58\n",
      "27\n",
      "41\n",
      "['/wiki/Unique_factorization_domain', '/wiki/Constructive_proof', '/wiki/Bruno_Buchberger']\n",
      "/wiki/Unique_factorization_domain\n",
      "10\n",
      "['/wiki/Polynomial_ring']\n",
      "/wiki/Constructive_proof\n",
      "49\n",
      "42\n",
      "['/wiki/Axiom_of_choice', '/wiki/Graph_(discrete_mathematics)']\n",
      "/wiki/Bruno_Buchberger\n",
      "6\n",
      "3\n",
      "11\n",
      "7\n",
      "['/wiki/University_of_Linz', '/wiki/Ph.D._thesis', 'https://commons.wikimedia.org/wiki/Category:Bruno_Buchberger', '/wiki/Research_Institute_for_Symbolic_Computation']\n",
      "[['/wiki/Polynomial_ring'], ['/wiki/Axiom_of_choice', '/wiki/Graph_(discrete_mathematics)'], ['/wiki/University_of_Linz', '/wiki/Ph.D._thesis', 'https://commons.wikimedia.org/wiki/Category:Bruno_Buchberger', '/wiki/Research_Institute_for_Symbolic_Computation']]\n",
      "/wiki/Institute_for_Scientific_Information\n",
      "14\n",
      "17\n",
      "10\n",
      "['/wiki/Engineering', '/wiki/Physics', '/wiki/Journal_Citation_Reports']\n",
      "/wiki/Engineering\n",
      "46\n",
      "13\n",
      "['/wiki/Neo-Assyrian', '/wiki/Egyptian_pyramids']\n",
      "/wiki/Physics\n",
      "168\n",
      "187\n",
      "67\n",
      "166\n",
      "['/wiki/The_Road_to_Reality', '/wiki/Pneumatics', '/wiki/Byzantine_Empire', '/wiki/Platonism']\n",
      "/wiki/Journal_Citation_Reports\n",
      "3\n",
      "['/wiki/Web_of_Science']\n",
      "[['/wiki/Neo-Assyrian', '/wiki/Egyptian_pyramids'], ['/wiki/The_Road_to_Reality', '/wiki/Pneumatics', '/wiki/Byzantine_Empire', '/wiki/Platonism'], ['/wiki/Web_of_Science']]\n",
      "/wiki/Science_Citation_Index\n",
      "7\n",
      "1\n",
      "7\n",
      "['/wiki/Technology', '/wiki/Institute_for_Scientific_Information', '/wiki/Technology']\n",
      "/wiki/Technology\n",
      "298\n",
      "169\n",
      "['/wiki/Ted_Kaczynski', '/wiki/Physics']\n",
      "/wiki/Institute_for_Scientific_Information\n",
      "7\n",
      "1\n",
      "1\n",
      "8\n",
      "['/wiki/Social_Sciences_Citation_Index', '/wiki/Philadelphia', '/wiki/Philadelphia', '/wiki/Arts_and_Humanities_Citation_Index']\n",
      "/wiki/Technology\n",
      "235\n",
      "135\n",
      "['/wiki/Software_engineering', '/wiki/Windmill']\n",
      "[['/wiki/Ted_Kaczynski', '/wiki/Physics'], ['/wiki/Social_Sciences_Citation_Index', '/wiki/Philadelphia', '/wiki/Philadelphia', '/wiki/Arts_and_Humanities_Citation_Index'], ['/wiki/Software_engineering', '/wiki/Windmill']]\n",
      "/wiki/Institute_for_Scientific_Information\n",
      "18\n",
      "11\n",
      "15\n",
      "['/wiki/Academic_Ranking_of_World_Universities', '/wiki/Impact_factor', '/wiki/Biology']\n",
      "/wiki/Academic_Ranking_of_World_Universities\n",
      "7\n",
      "1\n",
      "['/wiki/University_of_Oxford', '/wiki/Shanghai_Jiao_Tong_University']\n",
      "/wiki/Impact_factor\n",
      "41\n",
      "12\n",
      "['/wiki/European_Association_of_Science_Editors', '/wiki/Onex_Corporation']\n",
      "/wiki/Biology\n",
      "1061\n",
      "740\n",
      "182\n",
      "['/wiki/Insecta', '/wiki/An_Essay_on_the_Principle_of_Population', '/wiki/Thiol']\n",
      "[['/wiki/University_of_Oxford', '/wiki/Shanghai_Jiao_Tong_University'], ['/wiki/European_Association_of_Science_Editors', '/wiki/Onex_Corporation'], ['/wiki/Insecta', '/wiki/An_Essay_on_the_Principle_of_Population', '/wiki/Thiol']]\n",
      "/wiki/Science_Citation_Index\n",
      "7\n",
      "4\n",
      "4\n",
      "['/wiki/Technology', '/wiki/Thomson_Reuters', '/wiki/Thomson_Reuters']\n",
      "/wiki/Technology\n",
      "105\n",
      "100\n",
      "['/wiki/Maykop_culture', '/wiki/Mesopotamia']\n",
      "/wiki/Thomson_Reuters\n",
      "34\n",
      "63\n",
      "25\n",
      "['/wiki/Reuters_3000_Xtra', '/wiki/Breakingviews', '/wiki/Sweet_%26_Maxwell']\n",
      "/wiki/Thomson_Reuters\n",
      "37\n",
      "11\n",
      "['/wiki/New_York_Stock_Exchange', '/wiki/The_Woodbridge_Company']\n",
      "[['/wiki/Maykop_culture', '/wiki/Mesopotamia'], ['/wiki/Reuters_3000_Xtra', '/wiki/Breakingviews', '/wiki/Sweet_%26_Maxwell'], ['/wiki/New_York_Stock_Exchange', '/wiki/The_Woodbridge_Company']]\n"
     ]
    }
   ],
   "source": [
    "for i in new_rooms:\n",
    "    for j in i:\n",
    "        dungeon_crawler(j, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6e571397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "354\n",
      "115\n",
      "72\n",
      "204\n",
      "['/wiki/K%C3%B6ppen_climate_classification', '/wiki/Twin_towns_and_sister_cities', '/wiki/Pope_Paul_III', '/wiki/Jos%C3%A9_Gustavo_Guerrero', '/wiki/Nice_Carnival']\n",
      "230\n",
      "338\n",
      "15\n",
      "232\n",
      "100\n",
      "['/wiki/Heraldic', '/wiki/Wikipedia:Citation_needed', '/wiki/French_Riviera', '/wiki/House_of_Savoy', '/wiki/Castle_of_Nice#Castle_Hill']\n",
      "131\n",
      "57\n",
      "268\n",
      "319\n",
      "351\n",
      "['/wiki/Treaty_of_Turin_(1860)', '/wiki/Dmitry_Shcherbachev', '/wiki/Corsica_Ferries_-_Sardinia_Ferries', '/wiki/Mus%C3%A9e_Matisse_(Nice)', '/w/index.php?title=Farcis_ni%C3%A7ois&action=edit&redlink=1']\n",
      "174\n",
      "138\n",
      "128\n",
      "191\n",
      "173\n",
      "['/wiki/Uruguay', '/wiki/Piedmont-Sardinia', '/wiki/Commune_in_France', '/wiki/UNESCO', '/wiki/Wikipedia:Citation_needed']\n",
      "7\n",
      "352\n",
      "259\n",
      "310\n",
      "27\n",
      "['/wiki/Italian_language', '/wiki/Salade_ni%C3%A7oise', '/wiki/Sophia_Antipolis', '/wiki/Matisse', '/wiki/Terra_Amata_(archaeological_site)']\n"
     ]
    }
   ],
   "source": [
    "for i in randomlinks:\n",
    "\n",
    "    testR = requests.get(\"https://en.wikipedia.org/wiki/Nice\").text\n",
    "    soupRtest = BeautifulSoup(testR, \"lxml\")\n",
    "    ## I grab a page off Wikipedia.\n",
    "\n",
    "    from random import *\n",
    "    ## I need an arbitrary function from random.\n",
    "\n",
    "    sectionRtest = soupRtest.find_all(\"p\")\n",
    "\n",
    "    a_Rtitle = []\n",
    "    ## a_title is a list of every <a> tag in every paragraph of the page, HTML format.\n",
    "    for i in sectionRtest:\n",
    "        try:\n",
    "            a_Rtitle.append(i.find_all(\"a\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        a_Rlinks = []\n",
    "        ## a_links is a list of every link pulled from the <a> strings in a_title.\n",
    "        for i in a_Rtitle:\n",
    "            for j in i:\n",
    "                if j.get(\"title\") == None:\n",
    "                    pass\n",
    "                else:\n",
    "                    a_Rlinks.append(j.get(\"href\"))\n",
    "                    \n",
    "    randomRlinks = []\n",
    "    x = 0\n",
    "    while x != 5:\n",
    "        random = randint(0,len(a_Rlinks)-1)\n",
    "        print(random)\n",
    "        randomRlinks.append(a_Rlinks[random])\n",
    "        x+= 1\n",
    "\n",
    "    print(randomRlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99756c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testvariable = ['/w/', 'disambiguation']\n",
    "bingus = \"akjdhgasdhkjww/w/\"\n",
    "\n",
    "any(i in bingus for i in testvariable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
